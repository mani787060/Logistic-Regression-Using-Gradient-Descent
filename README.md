# Logistic Regression Using Gradient Descent

This project implements **Logistic Regression from scratch** using **Gradient Descent** instead of built-in sklearn solvers.  
It covers the complete workflow from the sigmoid function to calculating loss and manually updating weights.

## Contents:-
- Sigmoid function implementation  
- Binary cross-entropy loss  
- Gradient Descent weight updates  
- Training loop from scratch  
- Decision boundary visualization  
- Comparison with sklearnâ€™s LogisticRegression

## Key Concepts:-
- Why logistic regression requires sigmoid  
- How gradients are derived for BCE loss  
- Convergence behavior based on learning rate

## Libraries Used:-
- NumPy  
- Matplotlib  
- Scikit-learn (for comparison)
